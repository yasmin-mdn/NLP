{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toLCaLuS4UXH"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "rxEc_HuUUYA4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bf98f91-7026-4e39-e0ff-87d9a1bb228c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import treebank\n",
        "import pprint\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.pipeline import Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhCrh5J94UXW"
      },
      "source": [
        "# Datasets download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2O4WyhH9WL2f",
        "outputId": "9971f469-7b45-4c32-e5cc-2a5d62e56f33",
        "scrolled": false
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Package treebank is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "nltk.download('treebank')\n",
        "nltk.download('punkt')\n",
        "nltk.download('universal_tagset')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9h2uUX14UXZ"
      },
      "source": [
        "# Tagged sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "3GYqC2mfUj_S"
      },
      "outputs": [],
      "source": [
        "tagged_sentences = nltk.corpus.treebank.tagged_sents()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def features(sentence, index):\n",
        "  return {  \n",
        "  'word': sentence[index], \n",
        "  'is_first': index == 0, \n",
        "  'is_last': index == len(sentence) - 1,    \n",
        "  'is_capitalized': sentence[index][0].upper() == sentence[index][0],     \n",
        "   'is_all_caps': sentence[index].upper() == sentence[index],    \n",
        "  'is_all_lower': sentence[index].lower() == sentence[index],    \n",
        "  'prefix-1': sentence[index][0],       \n",
        "  'prefix-2': sentence[index][:2],      \n",
        "  'prefix-3': sentence[index][:3],       \n",
        "  'suffix-1': sentence[index][-1],     \n",
        "  'suffix-2': sentence[index][-2:],      \n",
        "  'suffix-3': sentence[index][-3:],     \n",
        "  'prev_word': '' if index == 0 else sentence[index - 1],    \n",
        "  'next_word': '' if index == len(sentence) - 1 else sentence[index + 1],   \n",
        "  'has_hyphen': '-' in sentence[index],  \n",
        "  'is_numeric': sentence[index].isdigit(),   \n",
        "  'capitals_inside': sentence[index][1:].lower() != sentence[index][1:] \n",
        "   }  "
      ],
      "metadata": {
        "id": "nmze8iFy5a6A"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_to_dataset(sentences):  \n",
        "  X, y = [],[]    \n",
        "  for tagged in sentences:    \n",
        "    i=0\n",
        "    while(i<len(tagged)):   \n",
        "      X.append(features([w for w , t in tagged],i))              \n",
        "      y.append(tagged[i][1]) \n",
        "      i+=1      \n",
        "  return X, y  "
      ],
      "metadata": {
        "id": "RvBHaX966yuT"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "part=int(0.8 * len(tagged_sentences)) \n",
        "x_train, y_train = transform_to_dataset( tagged_sentences[:part])\n",
        "x_test, y_test = transform_to_dataset( tagged_sentences[part:])"
      ],
      "metadata": {
        "id": "PB-61PPNAf9-"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier=Pipeline([\n",
        "                     ('vectorizer', DictVectorizer(sparse=False)),\n",
        "                     ('classifier', DecisionTreeClassifier(criterion='entropy'))\n",
        "                     ]) \n",
        "classifier.fit(x_train[:10000], y_train[:10000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z6Y_J7Ro7qKg",
        "outputId": "fd2fe7f3-33f9-44df-9055-dd8c8bc6b90f"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train acc: 0.8961270880613118\n",
            "test acc 0.8937072708218973\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def pos_tag(sentence): \n",
        "  tags = classifier.predict([features(sentence, index) for index in range(len(sentence))]) \n",
        "  return list(zip(sentence, tags) )"
      ],
      "metadata": {
        "id": "QXVa4jjLDW4J"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"train acc:\",classifier.score(x_train, y_train))\n",
        "print(\"test acc\",classifier.score(x_test, y_test))"
      ],
      "metadata": {
        "id": "fIEkemwP3w2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UUrtLOR4UXb"
      },
      "source": [
        "# Sample for output of your PoS tagger"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WlBIyChncJIZ",
        "outputId": "2a52cb2a-e10d-48d1-94e4-1f324ce0dcb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('This', 'DT'), ('is', 'VBZ'), ('my', 'NN'), ('friend', 'NN'), (',', ','), ('John', 'NNP'), ('.', '.')]\n"
          ]
        }
      ],
      "source": [
        "print(list(pos_tag(word_tokenize('This is my friend, John.'))))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(list(pos_tag(word_tokenize(\"let's go shopping\"))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qgx58HM8BDqc",
        "outputId": "4c7423b1-3f43-45f7-e26f-784d8b8a71cb"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('let', 'VBD'), (\"'s\", 'POS'), ('go', 'NN'), ('shopping', 'VBG')]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "PoS_Tagger.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}